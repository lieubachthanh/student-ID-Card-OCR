{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"data\\VietOCR\\labels.txt\", delimiter = '\\t',names=['images', 'labels'], quoting=csv.QUOTE_NONE,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['images']=df['images'].apply(lambda x: x.replace('.jpg.jpg','.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = \"aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, lbl in df_train.values:\n",
    "    check = True\n",
    "    for i in lbl:\n",
    "        if i not in vocab:\n",
    "            check = False\n",
    "            break\n",
    "    if check:\n",
    "        img = \"images/\"+img\n",
    "        text = img + \"\\t\" + lbl + \"\\n\"\n",
    "        with open('data/VietOCR/train.txt', \"a\", encoding='utf-8') as f:\n",
    "            f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, lbl in df_test.values:\n",
    "    check = True\n",
    "    for i in lbl:\n",
    "        if i not in vocab:\n",
    "            check = False\n",
    "            break\n",
    "    if check:\n",
    "        img = \"images/\"+img\n",
    "        text = img + \"\\t\" + lbl + \"\\n\"\n",
    "        with open('data/VietOCR/test.txt', \"a\", encoding='utf-8') as f:\n",
    "            f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tool.predictor import Predictor\n",
    "from tool.config import Cfg\n",
    "from model.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Cfg.load_config_from_file('vgg_transformer')\n",
    "dataset_params = {\n",
    "    'name':'VietOCR',\n",
    "    'data_root':'data/VietOCR/',\n",
    "    'train_annotation':'train.txt',\n",
    "    'valid_annotation':'test.txt'\n",
    "}\n",
    "\n",
    "params = {\n",
    "         'print_every':200,\n",
    "         'valid_every':15*200,\n",
    "          'iters':10000,\n",
    "          'checkpoint':'./checkpoint/transformerocr_checkpoint.pth',    \n",
    "          'export':'./weights/transformerocr.pth',\n",
    "          'metrics': 40000\n",
    "         }\n",
    "config['vocab'] = config['vocab']\n",
    "config['trainer'].update(params)\n",
    "config['dataset'].update(dataset_params)\n",
    "config['device'] = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab': 'aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ ',\n",
       " 'device': 'cpu',\n",
       " 'seq_modeling': 'transformer',\n",
       " 'transformer': {'d_model': 256,\n",
       "  'nhead': 8,\n",
       "  'num_encoder_layers': 6,\n",
       "  'num_decoder_layers': 6,\n",
       "  'dim_feedforward': 2048,\n",
       "  'max_seq_length': 1024,\n",
       "  'pos_dropout': 0.1,\n",
       "  'trans_dropout': 0.1},\n",
       " 'optimizer': {'max_lr': 0.0003, 'pct_start': 0.1},\n",
       " 'trainer': {'batch_size': 32,\n",
       "  'print_every': 200,\n",
       "  'valid_every': 3000,\n",
       "  'iters': 10000,\n",
       "  'export': './weights/transformerocr.pth',\n",
       "  'checkpoint': './checkpoint/transformerocr_checkpoint.pth',\n",
       "  'log': './train.log',\n",
       "  'metrics': 40000},\n",
       " 'dataset': {'name': 'VietOCR',\n",
       "  'data_root': 'data/VietOCR/',\n",
       "  'train_annotation': 'train.txt',\n",
       "  'valid_annotation': 'test.txt',\n",
       "  'image_height': 32,\n",
       "  'image_min_width': 32,\n",
       "  'image_max_width': 512},\n",
       " 'dataloader': {'num_workers': 3, 'pin_memory': True},\n",
       " 'aug': {'image_aug': False, 'masked_language_model': True},\n",
       " 'predictor': {'beamsearch': False},\n",
       " 'quiet': False,\n",
       " 'pretrain': 'https://vocr.vn/data/vietocr/vgg_transformer.pth',\n",
       " 'weights': 'https://vocr.vn/data/vietocr/vgg_transformer.pth',\n",
       " 'backbone': 'vgg19_bn',\n",
       " 'cnn': {'pretrained': True,\n",
       "  'ss': [[2, 2], [2, 2], [2, 1], [2, 1], [1, 1]],\n",
       "  'ks': [[2, 2], [2, 2], [2, 1], [2, 1], [1, 1]],\n",
       "  'hidden': 256}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1D_JHg0P2BKUTVRqOS00EkLonWEIJKxGh?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maxsys\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Maxsys\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Maxsys\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "train_VietOCR: There is not enough space on the disk.\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32md:\\ACADEMIC\\AI8_FALL23\\DBM301\\capstone\\trOCR.ipynb Cell 13\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/ACADEMIC/AI8_FALL23/DBM301/capstone/trOCR.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(config, pretrained\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32md:\\ACADEMIC\\AI8_FALL23\\DBM301\\capstone\\model\\trainer.py:78\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, config, pretrained, augmentor)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_aug:\n\u001b[0;32m     76\u001b[0m     transforms \u001b[39m=\u001b[39m  augmentor\n\u001b[1;32m---> 78\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_gen(\u001b[39m'\u001b[39;49m\u001b[39mtrain_\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_name), \n\u001b[0;32m     79\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_root, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_annotation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmasked_language_model, transform\u001b[39m=\u001b[39;49mtransforms)\n\u001b[0;32m     80\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_annotation:\n\u001b[0;32m     81\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_gen(\u001b[39m'\u001b[39m\u001b[39mvalid_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_name), \n\u001b[0;32m     82\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_root, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_annotation, masked_language_model\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\ACADEMIC\\AI8_FALL23\\DBM301\\capstone\\model\\trainer.py:310\u001b[0m, in \u001b[0;36mTrainer.data_gen\u001b[1;34m(self, lmdb_path, data_root, annotation, masked_language_model, transform)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdata_gen\u001b[39m(\u001b[39mself\u001b[39m, lmdb_path, data_root, annotation, masked_language_model\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, transform\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 310\u001b[0m     dataset \u001b[39m=\u001b[39m OCRDataset(lmdb_path\u001b[39m=\u001b[39;49mlmdb_path, \n\u001b[0;32m    311\u001b[0m             root_dir\u001b[39m=\u001b[39;49mdata_root, annotation_path\u001b[39m=\u001b[39;49mannotation, \n\u001b[0;32m    312\u001b[0m             vocab\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab, transform\u001b[39m=\u001b[39;49mtransform, \n\u001b[0;32m    313\u001b[0m             image_height\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mimage_height\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[0;32m    314\u001b[0m             image_min_width\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mimage_min_width\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[0;32m    315\u001b[0m             image_max_width\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mimage_max_width\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    317\u001b[0m     sampler \u001b[39m=\u001b[39m ClusterRandomSampler(dataset, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    318\u001b[0m     collate_fn \u001b[39m=\u001b[39m Collator(masked_language_model)\n",
      "File \u001b[1;32md:\\ACADEMIC\\AI8_FALL23\\DBM301\\capstone\\loader\\dataloader.py:39\u001b[0m, in \u001b[0;36mOCRDataset.__init__\u001b[1;34m(self, lmdb_path, root_dir, annotation_path, vocab, image_height, image_min_width, image_max_width, transform)\u001b[0m\n\u001b[0;32m     37\u001b[0m     sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mflush()\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     createDataset(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlmdb_path, root_dir, annotation_path)\n\u001b[0;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m lmdb\u001b[39m.\u001b[39mopen(\n\u001b[0;32m     42\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlmdb_path,\n\u001b[0;32m     43\u001b[0m     max_readers\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     readahead\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     47\u001b[0m     meminit\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     48\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtxn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mbegin(write\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\ACADEMIC\\AI8_FALL23\\DBM301\\capstone\\tool\\create_dataset.py:47\u001b[0m, in \u001b[0;36mcreateDataset\u001b[1;34m(outputPath, root_dir, annotation_path)\u001b[0m\n\u001b[0;32m     44\u001b[0m     annotations \u001b[39m=\u001b[39m [l\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lines]\n\u001b[0;32m     46\u001b[0m nSamples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(annotations)\n\u001b[1;32m---> 47\u001b[0m env \u001b[39m=\u001b[39m lmdb\u001b[39m.\u001b[39;49mopen(outputPath, map_size\u001b[39m=\u001b[39;49m\u001b[39m1099511627776\u001b[39;49m)\n\u001b[0;32m     48\u001b[0m cache \u001b[39m=\u001b[39m {}\n\u001b[0;32m     49\u001b[0m cnt \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mError\u001b[0m: train_VietOCR: There is not enough space on the disk.\r\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(config, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.precision()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
